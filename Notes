For the existing Trash/Celery dashboard in Grafana:

Singlestat round guauge to show workers:  (Gauge)
sum(celery_workers{job=~"$job"})   (Singlestat round gauge)         (/api/tasks:  name and state?)

Summary task by status: (Gauge)
sum(rate(celery_tasks{job=~"$job"}[$TIME_FRAME])) by (state)

Task Runtime summary:  (histogram)
rate(celery_tasks_runtime_seconds_sum{job=~"$job"}[$TIME_FRAME])     (/api/tasks: runtime)

Task Latency:  (histogram)
rate(celery_task_latency_sum{job=~"$job"}[$TIME_FRAME])              (/api/tasks: ???)
rate(celery_task_latency_count{job=~"$job"}[$TIME_FRAME])            (/api/tasks: ???)

Top 15 Active tasks:
topk(15, sum(rate(celery_tasks_by_name{job=~"$job", state='STARTED'}[$TIME_FRAME])) by (app, name))     (/api/task: state, name)
(same again for Received, Pending, Failure, Retry, and Success)




Soooo.... I'm building the Flower dashboard for Grafana:

I have:

1) CELERY_WORKERS (Gauge)  [app] with count   (This gives the worker's name)


2) CELERY_TASK_TYPES_BY_STATE ['task_type', 'state'] with count

rate(celery_task_types_by_state{job=~"$job",state="FAILURE"}[5m])
sum(celery_task_types_by_state{job=~"$job",state="FAILURE"}) by (task_type)
sum(celery_task_types_by_state{job=~"$job"}) by (task_type, status)   
sum(celery_task_types_by_state{job=~"$job", state="RECEIVED"}) by (task_type)   <<<<<<<<<<<<<<


3) CELERY_TASKS_BY_NAME ['name', 'state'] with count.  So we can see what is in:
	FAILURE, PENDING, RECEIVED, RETRY, REVOKED, STARTED, and SUCCESS

4) CELERY_TASK_DURATION_BY_STATE ['name', 'runtime', 'state'].set_get_current_time()


Next:

Latency for RECEIVED and PENDING? RETRY?

Runtime for STARTED?

CELERY_TASK_DURATION_BY_STATE ['name', 'state', 'runtime']   
    histogram going into duration buckets, or gauge with `.set_to_current_time()` ???


  histogram??
rate(celery_tasks_runtime_seconds_sum{job=~"$job"}[$TIME_FRAME])

https://prometheus.io/docs/practices/histograms/  (You would use buckets for say, runtimes incurred by various task types, as an example)
See "https://prometheus.io/docs/practices/histograms/#apdex-score"

https://www.robustperception.io/how-does-a-prometheus-histogram-work

Runtime goes into bucket?  Task-name, state, runtime.



Prom types: https://github.com/prometheus/client_python

rate(celery_task_types_by_state{job=~"$job",state="FAILURE"}[5m])

ISSUE WITH RESTARTS, need to be able to add a timeout to the API call.  Code will not allow this extra parameter to be added
as is written.  These restarts are the reason, I beleive, we see gaps in the grafana graphs.

[Requests Module Advanced Docs](https://requests.readthedocs.io/en/master/user/advanced/)

**Cheers!**


Possible restart for "complete":
================================

Exception in thread Thread-2:
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/urllib3/response.py", line 362, in _error_catcher
    yield
  File "/usr/local/lib/python3.6/site-packages/urllib3/response.py", line 444, in read
    data = self._fp.read(amt)
  File "/usr/local/lib/python3.6/http/client.py", line 459, in read
    n = self.readinto(b)
  File "/usr/local/lib/python3.6/http/client.py", line 503, in readinto
    n = self.fp.readinto(b)
  File "/usr/local/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/requests/models.py", line 750, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/local/lib/python3.6/site-packages/urllib3/response.py", line 496, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "/usr/local/lib/python3.6/site-packages/urllib3/response.py", line 461, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File "/usr/local/lib/python3.6/contextlib.py", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/lib/python3.6/site-packages/urllib3/response.py", line 380, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
urllib3.exceptions.ProtocolError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/app/celery_task_types_by_state_monitor.py", line 67, in run
    self.get_metrics()
  File "/app/celery_task_types_by_state_monitor.py", line 38, in get_metrics
    data = req_session.send(request_prepped, timeout=(3, 15))
  File "/usr/local/lib/python3.6/site-packages/requests/sessions.py", line 686, in send
    r.content
  File "/usr/local/lib/python3.6/site-packages/requests/models.py", line 828, in content
    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''
  File "/usr/local/lib/python3.6/site-packages/requests/models.py", line 753, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

